{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as p\n",
    "from itertools import combinations\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I: Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de error cuadrático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(w, X, Y):\n",
    "    N = X.shape[0]\n",
    "    E = 0\n",
    "    for i in range(N):\n",
    "        E += (np.dot(X[i],w) - Y[i])**2\n",
    "    return E[0]/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de descenso de gradiente\n",
    "def gradient_descent(X, Y, weights, learning_rate):\n",
    "\n",
    "    dldw = np.zeros((len(weights), 1))\n",
    "    N = X.shape[0] # número de filas de la matriz X\n",
    "\n",
    "    # Calculo de la derivada parcial de loss con respecto a cada peso\n",
    "    for i in range(N):\n",
    "        dldw += np.transpose([X[i]]) * ((Y[i] - np.dot(X[i], weights))[0])\n",
    "\n",
    "    weights = weights + learning_rate * dldw\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacion de funcion linear aleatoria de tamaño determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de Parámetros\n",
    "def generate_linear_function(size):\n",
    "    random_seed = 1\n",
    "    weight_range = 100\n",
    "    x_range = 100 \n",
    "    number_seeds = 1000\n",
    "    \n",
    "    weights = np.random.choice(np.concatenate((np.arange(-weight_range, 0), np.arange(1, weight_range+1))), size=(size, 1)) #pesos a estimar\n",
    "    X0 = np.ones((number_seeds, 1)) #parametro X0 equivalente a 1 en cada vector X add \n",
    "    X1toN = np.random.randint(1, x_range+1, size=(number_seeds, size-1))  \n",
    "    X = np.concatenate((X1toN, X0), axis=1)# X = conjunto de variables independientes\n",
    "\n",
    "    #Y = X1toN*w # + w0 que no consideramos por no ser significativa\n",
    "    Y = np.dot(X,weights) \n",
    "\n",
    "    return weights, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(X, Y, size, learning_rate, max_iter, epsilon, w=[]):\n",
    "    if len(w) == 0: w = np.ones((size,1)) #pesos iniciales\n",
    "    error = []\n",
    "\n",
    "    #Iteración del algoritmo \n",
    "    for i in range(max_iter):\n",
    "        w = gradient_descent(X, Y, w, learning_rate)\n",
    "        err = error_rate(w, X, Y)\n",
    "        error.append(err)\n",
    "        if err < epsilon:\n",
    "            break\n",
    "    return w, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la funcion de aprendizaje con una funcion linear aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = 6\n",
    "function_weights, X, Y = generate_linear_function(size) \n",
    "w, error = learner(X, Y, size, 0.00000001, 1000, 0.5)\n",
    "\n",
    "p.plot([i+1 for i in range(len(error))], error)\n",
    "p.xlabel(\"Iteraciones\")\n",
    "p.ylabel(\"Error cuadrático\")\n",
    "p.show()\n",
    "\n",
    "print(f\"pesos reales: {np.transpose(function_weights)}\")\n",
    "print(f\"pesos estimados: {np.transpose(w)}\")\n",
    "\n",
    "def porcentage_error(real, estimated):\n",
    "    return np.abs(((estimated * 100) / real) - 100)\n",
    "\n",
    "print(f\"porcentaje de error: {porcentage_error(function_weights, w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II: Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "old_df = pd.read_csv('CarDekho.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ver los valores nulos\n",
    "df_nulos = old_df.isnull().sum().reset_index()\n",
    "df_nulos.columns = ['variable', '# nulos']\n",
    "df_nulos = df_nulos[df_nulos['# nulos'] > 0]\n",
    "df_nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las tablas en el archivo .csv, aquellas con valores nulos son:\n",
    "* Engine\n",
    "* Max Power\n",
    "* Max Torque\n",
    "* Drivetrain\n",
    "* Length\n",
    "* Width\n",
    "* Height\n",
    "* Seaing Capacity\n",
    "* Fuel Tank Capacity\n",
    "\n",
    "En este caso, el manejo de datos faltantes se realizará de la siguiente manera:\n",
    "* **Valores numéricos:**  Se reemplazan por la mediana del atributo.\n",
    "* **Valores no numéricos:** Se reemplazan por el valor más común del atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = old_df\n",
    "\n",
    "# Manejo de datos faltantes para valores numéricos\n",
    "mean_length = df.loc[:, 'Length'].mean()\n",
    "mean_width = df.loc[:, 'Width'].mean()\n",
    "mean_height = df.loc[:, 'Height'].mean()\n",
    "mean_seats = df.loc[:, 'Seating Capacity'].mean()\n",
    "mean_tank = df.loc[:, 'Fuel Tank Capacity'].mean()\n",
    "\n",
    "#conversion de datos de la columna Engine a numéricos\n",
    "df['Engine'] = pd.to_numeric(df.loc[:,'Engine'].str.strip(\" cc\"), errors='coerce')\n",
    "mean_engine = df.loc[:, 'Engine'].mean()\n",
    "\n",
    "# Manejo de datos faltantes para valores categóricos\n",
    "common_drive = df.loc[:, 'Drivetrain'].value_counts().index[0]\n",
    "common_power = df.loc[:, 'Max Power'].value_counts().index[0]\n",
    "common_torque = df.loc[:, 'Max Torque'].value_counts().index[0]\n",
    "\n",
    "# Reemplazo de valores nulos\n",
    "df.loc[:, 'Length'] = df.loc[:, 'Length'].fillna(mean_length)\n",
    "df.loc[:, 'Width'] = df.loc[:, 'Width'].fillna(mean_width)\n",
    "df.loc[:, 'Height'] = df.loc[:, 'Height'].fillna(mean_height)\n",
    "df.loc[:, 'Seating Capacity'] = df.loc[:, 'Seating Capacity'].fillna(mean_seats)\n",
    "df.loc[:, 'Fuel Tank Capacity'] = df.loc[:, 'Fuel Tank Capacity'].fillna(mean_tank)\n",
    "df.loc[:, 'Drivetrain'] = df.loc[:, 'Drivetrain'].fillna(common_drive)\n",
    "df.loc[:, 'Engine'] = df.loc[:, 'Engine'].fillna(mean_engine)\n",
    "df.loc[:, 'Max Power'] = df.loc[:, 'Max Power'].fillna(common_power)\n",
    "df.loc[:, 'Max Torque'] = df.loc[:, 'Max Torque'].fillna(common_torque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del boxplot, se observa que las columnas *Price* y *Kilometer* tienen valores mucho mayores que las demás variables. Por ende, se normalizan todas las columnas numéricas para que sus valores estén entre 0 y 1 y den datos más concisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Normalización de datos\n",
    "df.loc[:, 'Price'] = normalize_column(df.loc[:, 'Price'])\n",
    "df.loc[:, 'Year'] = normalize_column(df.loc[:, 'Year'])\n",
    "df.loc[:, 'Kilometer'] = normalize_column(df.loc[:, 'Kilometer'])\n",
    "df.loc[:, 'Engine'] = normalize_column(df.loc[:, 'Engine'])\n",
    "df.loc[:, 'Length'] = normalize_column(df.loc[:, 'Length'])\n",
    "df.loc[:, 'Width'] = normalize_column(df.loc[:, 'Width'])\n",
    "df.loc[:, 'Height'] = normalize_column(df.loc[:, 'Height'])\n",
    "df.loc[:, 'Seating Capacity'] = normalize_column(df.loc[:, 'Seating Capacity'])\n",
    "df.loc[:, 'Fuel Tank Capacity'] = normalize_column(df.loc[:, 'Fuel Tank Capacity'])\n",
    "\n",
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas _Max Power_ y _Max Torque_ serán eliminadas debido a la dificultad de estandarizar estos datos como valor numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Max Power', axis=1)\n",
    "df = df.drop('Max Torque', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos entonces 17 atributos con los que trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = list(df.loc[:, df.columns != \"Price\"].columns)\n",
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Finalmente se utiliza la función get_dummies de la librería Panda para crear nuevas $n$ columnas para cada uno de los atributos categóricos de $n$ categorías que tenemos\n",
    "\n",
    "También creamos una lista ```indexes```, que continene en cada índice $i$, los $j$ índices de las columnas dummy creadas en base a la $i$ columna de la data original, que será utilizada más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = [ len(pd.get_dummies(df[[col]]).columns) for col in df.loc[:, df.columns != \"Price\"].columns]\n",
    "indexes = [list(range(a[0]))] + [list(range(sum(a[0:i]), sum(a[0:i]) +  a[i])) for i in range(1,len(a))]\n",
    "\n",
    "df = pd.get_dummies(df, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mantener los valores pre-procesados guardados, creamos un nuevo csv que es con el que estaremos trabajando de aquí en adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_data.csv', index=False)\n",
    "df = pd.read_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III: Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccion de columnas\n",
    "\n",
    "La selección de columnas a utilizar en los vectores de ejemplo para el el entrenamiento del modelo será mediante un hiperparámetro que definirá las columnas a utilizar. Para esto calcularemos las combinaciones posibles de columnas, tal que no se permuten y se mantenga el orden relativo entre ellas. Luego generamos un modelo para cada una, y escogemos la combinación con el menor error cuadrático estimado\n",
    "\n",
    "Por cuestiones de tiempo, y puesto que es muy probable que con significativamente menos atributos el modelo esté subajustado, solo se considerarán las combinaciones de tamaño 8 y 9. Las combinaciones son guardadas en un archivo para eliminar el tiempo de computación de estas durante la selección del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_size in range(8,9):\n",
    "    cs = []\n",
    "    for c in (list(c) for c in combinations(range(len(attributes)), c_size)):\n",
    "        cs.append(c)\n",
    "    f = open(f\"c{str(c_size)}.txt\", \"w\")\n",
    "    f.write(str(cs))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos ahora las funciones para validación cruzada y selección de modelo. Se toman como hiperparámetros la cantidad $k$ de subconjutos de partición para la validación cruzada, el archivo de combinaciones a leer y dos indices ($a$, $b$) de la lista de combinaciones, para probar combinaciones solo en ese intervalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_selection(X, Y, k, learning_rate, max_iter, epsilon, txt, a, b):\n",
    "    #dividimos los ejemplos en dos partes, los conjuntos de entrenatmiento y el de prueba\n",
    "    np.random.seed()\n",
    "    s1 = np.random.choice(range(X.shape[0]), X.shape[0]*3//4, replace=False)\n",
    "    s2 = list(set(range(X.shape[0])) - set(s1))\n",
    "    training_X = X[s1, :]\n",
    "    training_Y = Y[s1, :]\n",
    "    test_X = X[s2, :]\n",
    "    test_Y = Y[s2, :]\n",
    "\n",
    "    combinations = []\n",
    "    errs = []\n",
    "\n",
    "    #Entrenamos con validación cruzada el modelo con un subconjunto de combinaciones dado\n",
    "    f = open(txt, \"r\")\n",
    "    column_combinations = json.loads(f.readline())[a:b]\n",
    "    f.close()\n",
    "    for c in column_combinations:\n",
    "        combinations.append(c)\n",
    "        #Utilizamos la lista indexes para asegurarnos que, al escoger una columna i de la data original, \n",
    "        #la combinación incluya los j índices de las columnas dummy correspondientes\n",
    "        Xc = training_X[:,sum([indexes[i] for i in c], [])] \n",
    "\n",
    "        errs.append(cross_validation(Xc, training_Y, k, learning_rate, max_iter, epsilon))\n",
    "\n",
    "    i = errs.index(min(errs))\n",
    "    c = combinations[i]\n",
    "    ids = sum([indexes[i] for i in c], [])\n",
    "    Xc = training_X[:,ids] \n",
    "\n",
    "    w = learner(Xc, Y, len(ids), learning_rate, max_iter, epsilon)[0]\n",
    "\n",
    "    test_Xc = test_X[:,ids]\n",
    "\n",
    "    return w, c, error_rate(w, test_Xc, test_Y)\n",
    "\n",
    "def cross_validation(X, Y, k,learning_rate, max_iter, epsilon):\n",
    "    N, w_size = X.shape\n",
    "    errs = 0\n",
    "    for i in range(k):\n",
    "        vdi = range((i-1)*N//k, i*N//k)\n",
    "        tsi = list(set(range(N)) - set(vdi))\n",
    "        validation_X = X[vdi, : ]\n",
    "        validation_Y = Y[vdi, : ]\n",
    "        training_X = X[tsi, :]\n",
    "        training_Y = Y[tsi, :]\n",
    "\n",
    "        w = learner(training_X, training_Y, w_size, learning_rate, max_iter, epsilon)[0]\n",
    "\n",
    "        errs += error_rate(w, validation_X, validation_Y)\n",
    "    \n",
    "    return errs/k\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la matriz vectorial X de los ejemplos para el entrenamiento, así como el vector Y correspondiente a cada vector de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df. columns != \"Price\"].to_numpy()\n",
    "Y =  df[[\"Price\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteramos ahora sobre las combinaciones de parámetros para seleccionar la combinación con menor error cuadrático estimado.\n",
    "\n",
    "Se probaron los intervalos [0,1079] U [2500,3120] en c8.txt y [0,2249] U [19540,19640] en c9.txt, con parámetros de aprendizaje que minimizaran el tiempo de corrida (numero de iteraciones y tasa de aprendizaje), para probar la mayor cantidad posible de combinaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection(X, Y, 5, 0.000002, 500, 0.15, \"c9.txt\", 3453,3454)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Luego, de las mejores combinaciones encontradas, guardadas en el archivo best.txt, se probó nuevamente utilizando parámetros de aprendizaje mas estrictos. De ahi escogemos la combinación final de columnas a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, c, e = model_selection(X, Y, 5, 0.0000005, 2000, 0.15,  \"best.txt\", None,None)\n",
    "([attributes[i] for i in c], e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos ahora al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [attributes.index(i) for i in ['Make',\n",
    "  'Model',\n",
    "  'Year',\n",
    "  'Kilometer',\n",
    "  'Fuel Type',\n",
    "  'Transmission',\n",
    "  'Drivetrain',\n",
    "  'Width',\n",
    "  'Seating Capacity']]\n",
    "\n",
    "ids = sum([indexes[i] for i in columns], [])\n",
    "\n",
    "Xc = X[:,ids] \n",
    "w, err = learner(Xc, Y, len(ids), 0.0000013, 160000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los resultados en ```w.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtxt = open(\"w.txt\", \"w\")\n",
    "wtxt.write(str(np.concatenate(w).tolist()))\n",
    "wtxt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
